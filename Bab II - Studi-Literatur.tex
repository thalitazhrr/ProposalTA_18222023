% ==============================================================================
% BAB II - STUDI LITERATUR 
% ==============================================================================

\chapter{STUDI LITERATUR}

Bab ini menyajikan kajian teoretis dan tinjauan pustaka yang relevan dengan topik penelitian. Pembahasan meliputi konsep \textit{Business Intelligence}, teknologi \textit{chatbot} dan \textit{Conversational AI}, klasifikasi \textit{intent} berbasis pola, pembangkitan bahasa alami berbasis templat, serta peramalan deret waktu yang mendukung analisis dan prediksi permintaan layanan pelanggan. Kajian literatur ini disusun secara sistematis untuk memberikan landasan teoretis yang kuat bagi pengembangan sistem yang diusulkan, dengan fokus pada integrasi \textit{pattern-based intent classification}, \textit{template-based natural language generation}, dan \textit{time series forecasting} dalam satu platform terintegrasi.

% ==============================================================================
\section{\textit{Business Intelligence} dan Arsitektur Sistem}
% ==============================================================================

Bagian ini membahas konsep dasar \textit{Business Intelligence}, evolusinya, serta arsitektur sistem yang mendukung implementasi BI modern. Pembahasan mencakup komponen-komponen utama arsitektur BI dan peran \textit{semantic layer} dalam menjembatani kesenjangan antara data teknis dan kebutuhan bisnis. Pemahaman yang komprehensif tentang arsitektur BI menjadi fondasi penting dalam merancang sistem yang mampu mengintegrasikan berbagai komponen teknologi secara efektif.

\subsection{Konsep \textit{Business Intelligence}}

\textit{Business Intelligence} (BI) merupakan sistem teknologi yang mengumpulkan, mengorganisasi, dan menganalisis data dari berbagai sumber dalam suatu bisnis untuk membantu perusahaan mengubah data mentah menjadi wawasan yang berguna sehingga mereka dapat membuat keputusan yang lebih baik \parencite{sorour2020business}. Evolusi BI dari sistem berbasis aturan tradisional ke platform berbasis kecerdasan buatan telah meningkatkan secara signifikan analitik prediktif, otomatisasi, dan kemampuan pengambilan keputusan waktu nyata di seluruh industri \parencite{khaddam2025ai}.

Implementasi BI di institusi pendidikan tinggi telah menunjukkan kemampuan untuk memantau aktivitas jaminan kualitas dan mendukung pengambilan keputusan strategis \parencite{sorour2020business}. Penelitian \textcite{shah2025hybrid} menunjukkan bahwa organisasi yang mengimplementasikan arsitektur analitik hibrida melaporkan peningkatan kemampuan penanganan data sebesar 85\% dibandingkan sistem tradisional, dengan konsistensi dan standar kualitas data yang terjaga. Pasar BI global terus menunjukkan pertumbuhan yang signifikan, dengan proyeksi mencapai USD 53,8 miliar pada tahun 2033, meningkat dari USD 25,9 miliar pada tahun 2024, dengan CAGR sebesar 8,48\% \parencite{Straits2024}. Pertumbuhan ini menunjukkan relevansi dan kebutuhan akan solusi BI yang inovatif dalam menghadapi kompleksitas data bisnis modern.

\subsection{Arsitektur \textit{Business Intelligence} Hibrida}

Arsitektur BI hibrida mengintegrasikan sistem BI tradisional dengan kemampuan kecerdasan buatan untuk meningkatkan pengambilan keputusan, analitik prediktif, dan efisiensi operasional. Pendekatan ini menyajikan struktur yang memanfaatkan model pembelajaran mesin bersama pelaporan BI tradisional untuk menjembatani kesenjangan antara analisis historis dan wawasan berbasis data waktu nyata \parencite{shah2025hybrid}.

Komponen utama arsitektur BI hibrida mencakup beberapa lapisan yang saling terintegrasi \parencite{shah2025hybrid}. Lapisan sumber data mencakup sumber internal seperti sistem CRM dan perangkat lunak keuangan, serta sumber eksternal seperti data media sosial dan laporan pasar. Gudang data berfungsi sebagai fasilitas penyimpanan terpusat untuk data yang terorganisasi dan siap digunakan untuk pelaporan. Mesin analitik bertanggung jawab untuk pengenalan pola dan analisis tren, memberikan wawasan prediktif untuk perencanaan strategis. Lapisan visualisasi menyediakan dashboard interaktif dan laporan yang meningkatkan adopsi pengguna melalui presentasi data yang intuitif.

Implementasi arsitektur analitik hibrida menghasilkan peningkatan 27\% dalam proses pengambilan keputusan dan peningkatan 31\% dalam metrik kinerja organisasi secara keseluruhan \parencite{shah2025hybrid}. Integrasi komponen-komponen ini membentuk fondasi yang kuat untuk sistem BI modern yang responsif dan dapat diandalkan. Keberhasilan implementasi arsitektur hibrida ini membuka jalan bagi pengembangan sistem yang lebih kompleks dan terintegrasi dengan teknologi percakapan.

\subsection{\textit{Semantic Layer} dalam \textit{Business Intelligence}}

\textit{Semantic layer} adalah antarmuka berorientasi bisnis yang menjembatani kesenjangan antara model data yang kompleks dan pengguna bisnis, bertindak sebagai lapisan abstraksi yang menerjemahkan struktur data teknis ke dalam istilah dan konsep bisnis yang familiar \parencite{databricks2025semantic}. Lapisan semantik menyediakan pandangan bisnis terpadu terhadap data di seluruh organisasi, terlepas dari lokasi data atau bagaimana secara teknis terstruktur.

Lapisan semantik menyederhanakan konsep dan teknis data untuk pengguna bisnis sehingga mereka tidak perlu mengubah data bisnis yang mendasarinya untuk bekerja dengan cara baru \parencite{atscale2025semantic}. Lapisan ini memungkinkan pengguna bisnis untuk berinteraksi dengan data menggunakan terminologi yang mereka pahami tanpa perlu memahami struktur teknis basis data yang mendasarinya. Kemampuan ini sangat penting dalam konteks sistem \textit{chatbot} BI, dengan memfasilitasi interpretasi kueri bahasa alami menjadi operasi basis data yang sesuai.

\subsubsection{Komponen Inti \textit{Semantic Layer}}

Lapisan semantik memiliki lima komponen inti yang bertindak sebagai fondasi struktural dan teknis \parencite{dbt2024semantic}. Komponen pertama adalah definisi model semantik yang menciptakan representasi logis dari domain bisnis, memetakan struktur basis data teknis ke konsep bisnis. Alih-alih bekerja dengan tabel mentah seperti \texttt{usr\_tbl} atau \texttt{trx\_hist}, entitas seperti \texttt{Customer} atau \texttt{Order} didefinisikan untuk merangkum kompleksitas yang mendasarinya.

Komponen kedua adalah manajemen metadata yang menangani informasi tentang data, seperti deskripsi bidang, garis keturunan data (\textit{data lineage}), frekuensi pembaruan, dan metrik kualitas. Lapisan logika bisnis sebagai komponen ketiga berisi aturan perhitungan spesifik untuk metrik bisnis. Komponen keempat adalah lapisan akses data yang menerjemahkan permintaan bisnis menjadi kueri basis data yang dioptimalkan, menerapkan filter keamanan yang diperlukan. Komponen terakhir adalah mekanisme \textit{caching} yang memeriksa apakah hasil perhitungan sudah tersedia dalam tembolok sebelum menjalankan kueri.

Implementasi \textit{semantic layer} yang efektif memungkinkan organisasi untuk mencapai konsistensi dalam pelaporan dan analisis, sekaligus mengurangi kompleksitas teknis yang dihadapi oleh pengguna bisnis. Pemahaman tentang \textit{semantic layer} ini menjadi dasar penting dalam merancang sistem klasifikasi \textit{intent} yang akan dibahas pada bagian selanjutnya.

% ==============================================================================
\section{\textit{Pattern-Based Intent Classification} dengan \textit{Confidence Level}}
% ==============================================================================

\begin{figure}[H] 
  \centering
  \includegraphics[width=0.46\textwidth,
                   height=0.32\textheight,
                   keepaspectratio]{image/pbi.png}
  \caption{\textit{Pattern-Based Intent Classification}}
  \label{fig:pbi}
\end{figure}

Bagian ini membahas pendekatan klasifikasi \textit{intent} berbasis pola untuk sistem \textit{chatbot}, dengan fokus pada mekanisme pencocokan pola, penggunaan \textit{confidence score}, dan evaluasi performa. Klasifikasi \textit{intent} merupakan komponen kritis dalam memahami maksud pengguna dari kueri bahasa alami dan menentukan respons yang sesuai berdasarkan data internal perusahaan.

\subsection{Konsep \textit{Intent Classification}}

Klasifikasi \textit{intent} adalah proses menentukan niat atau tujuan pengguna dari masukan mereka, yang merupakan tugas fundamental dalam sistem \textit{chatbot} percakapan \parencite{ouaddi2025comparative}. Inti dari \textit{chatbot} berbasis AI adalah komponen NLU yang mengklasifikasikan \textit{intent} pengguna untuk menghasilkan respons yang sesuai. Tugas klasifikasi ini sangat penting karena menentukan alur percakapan dan tindakan yang akan diambil sistem.

Pasar \textit{Conversational AI} global diproyeksikan mencapai USD 49,9 miliar pada tahun 2030, meningkat dari USD 10,7 miliar pada tahun 2023, dengan CAGR sebesar 25,2\% \parencite{Qaltivate2025}. Pertumbuhan eksponensial ini didorong oleh meningkatnya adopsi teknologi AI dalam layanan pelanggan dan operasional bisnis. \textit{Chatbot} dapat menjawab hampir 80\% dari semua pertanyaan standar, yang secara signifikan mengurangi beban kerja agen manusia dan mempercepat waktu respons \parencite{Fullview2025}. Statistik ini menunjukkan potensi besar klasifikasi \textit{intent} dalam meningkatkan efisiensi operasional dan kepuasan pengguna.

\subsection{Klasifikasi \textit{Intent} Berbasis \textit{Transformer}}

Klasifikasi \textit{intent} berbasis \textit{Transformer} merupakan pendekatan modern yang memanfaatkan arsitektur jaringan saraf dengan mekanisme perhatian diri (\textit{self-attention}) untuk memahami konteks ujaran pengguna secara dua arah dan menyeluruh \parencite{Merugu2024IntentLLM}. Model-model seperti BERT (\textit{Bidirectional Encoder Representations from Transformers}), RoBERTa, IndoBERT, dan berbagai \textit{sentence transformer} telah menunjukkan kinerja yang unggul pada tugas klasifikasi teks, termasuk klasifikasi \textit{intent} dalam sistem percakapan \parencite{Ozerova2022IntentModule,Gehweiler2024IntentModeration}. Pendekatan ini merepresentasikan setiap kalimat sebagai vektor berdimensi tinggi yang memuat informasi semantik kaya, kemudian menambahkan lapisan klasifikasi di bagian akhir untuk memetakan representasi tersebut ke label \textit{intent} yang telah didefinisikan.

Berbagai penelitian melaporkan bahwa model berbasis \textit{Transformer} mampu melampaui pendekatan tradisional seperti LSTM, CNN, maupun metode berbasis fitur manual dalam hal akurasi, skor F1, dan kemampuan menangkap nuansa konteks \parencite{Gehweiler2024IntentModeration}. Studi yang membandingkan BERT, RoBERTa, dan IndoBERT untuk klasifikasi \textit{intent} \textit{chatbot} berbahasa Indonesia menunjukkan bahwa IndoBERT dapat mencapai akurasi hingga sekitar 0{,}94 berkat proses prapelatihan yang selaras dengan struktur bahasa Indonesia dan korpus yang digunakan. Penelitian lain yang memanfaatkan MiniLM dan RoBERTa untuk modul deteksi \textit{intent} dalam asisten percakapan melaporkan peningkatan akurasi 10--15 persen dibandingkan solusi sebelumnya, sekaligus menekankan pentingnya memperhatikan kecepatan inferensi dan kebutuhan perangkat keras saat model \textit{Transformer} diintegrasikan ke dalam sistem produksi \parencite{Ozerova2022IntentModule}.

Perkembangan model bahasa besar berbasis \textit{Transformer} juga mendorong kajian terbaru yang mengevaluasi penggunaan \textit{sentence transformer}, model BERT terkontrasif, dan bahkan model bahasa besar generatif untuk tugas deteksi \textit{intent} pada agen percakapan tugas-orientasi \parencite{Merugu2024IntentLLM,Gehweiler2025IntentSurvey}. Hasil kajian tersebut menyoroti adanya kompromi yang jelas antara kualitas prediksi, kemampuan menangani \textit{intent} di luar cakupan (\textit{out-of-scope}), latensi sistem, serta konsumsi sumber daya komputasi, sehingga pemilihan arsitektur perlu mempertimbangkan kebutuhan dan batasan operasional sistem yang dibangun \parencite{Merugu2024IntentLLM}.

Dalam konteks penelitian ini, klasifikasi intent berbasis \textit{Transformer} diposisikan sebagai pendekatan yang relevan dan dikaji secara teoretis, tetapi tidak diimplementasikan secara langsung pada prototipe sistem. Pertimbangan utama adalah ruang lingkup pertanyaan yang relatif terbatas pada kebutuhan internal, pola kueri yang cenderung berulang, serta prioritas untuk menjaga efisiensi pemrosesan dan menghemat penggunaan sumber daya komputasi berbiaya tinggi seperti unit pemroses grafis (GPU) \parencite{Ozerova2022IntentModule,Gehweiler2024IntentModeration}. Oleh karena itu, penelitian ini memilih pendekatan klasifikasi \textit{intent} berbasis pola dengan mekanisme nilai keyakinan (\textit{pattern-based intent classification with confidence scoring}) yang lebih ringan secara komputasi, sambil menjadikan pendekatan berbasis \textit{Transformer} sebagai acuan pengembangan lanjutan ketika cakupan \textit{intent} dan beban sistem meningkat di masa mendatang.

\subsection{\textit{Pattern Matching} untuk \textit{Intent Classification}}

Pendekatan berbasis pola (\textit{pattern matching}) dalam klasifikasi \textit{intent} menggunakan aturan dan templat yang telah ditentukan sebelumnya untuk mencocokkan masukan pengguna dengan kategori \textit{intent} yang sesuai. Sistem ini menyimpan pola-pola pertanyaan atau \textit{utterances} untuk setiap \textit{intent}, kemudian membandingkan input pengguna dengan pola-pola yang telah didefinisikan untuk menemukan kecocokan terbaik.

Pendekatan \textit{pattern matching} efektif untuk menangani kueri terstruktur dan berulang dalam domain spesifik. Implementasi dapat dilakukan menggunakan ekspresi reguler, pencocokan \textit{n-gram}, atau algoritma kesamaan string seperti \textit{Fuzzy String Matching} \parencite{mulyatun2021pendekatan}. Keuntungan utama pendekatan ini adalah kecepatan eksekusi yang tinggi, prediktabilitas hasil, dan kontrol penuh atas logika pencocokan, menjadikannya pilihan yang tepat untuk sistem yang memerlukan respons cepat dan deterministic based pada data internal perusahaan.

\subsection{\textit{Confidence Score} dalam \textit{Intent Classification}}

\textit{Confidence score} atau skor keyakinan adalah nilai numerik antara 0 dan 1 yang menunjukkan tingkat kepercayaan sistem terhadap prediksi klasifikasi \textit{intent} \parencite{khosla2022evaluating}. Skor ini dihitung berdasarkan tingkat kecocokan antara input pengguna dengan pola yang telah didefinisikan. Semakin tinggi kecocokan, semakin tinggi nilai \textit{confidence score}, dan semakin besar kemungkinan respons yang diberikan adalah benar.

Sistem menghitung \textit{confidence score} dengan membandingkan input pengguna terhadap semua pola \textit{utterances} atau frasa pelatihan yang tersedia. Sistem kemudian memilih \textit{intent} dengan skor tertinggi sebagai prediksi. Implementasi \textit{confidence score} memungkinkan sistem untuk mengidentifikasi kapan prediksi mungkin tidak akurat dan memerlukan mekanisme \textit{fallback} \parencite{kuligowska2024enhancing}.

\subsubsection{\textit{Confidence Threshold}}

\textit{Confidence threshold} adalah nilai ambang batas minimum yang harus dicapai oleh \textit{confidence score} agar sistem memicu respons \textit{intent} tertentu. Nilai threshold adalah angka numerik antara 0 dan 1, dengan nilai default yang umum digunakan adalah 0,45 atau 45\% \parencite{teneo2022intent}. Sistem yang lebih konservatif dapat menggunakan threshold lebih tinggi seperti 0,70 atau 70\% untuk mengurangi risiko respons yang salah.

Jika \textit{confidence score} terbaik jatuh di bawah threshold, sistem akan memicu interaksi \textit{fallback}, biasanya berupa pesan yang meminta klarifikasi atau memberitahu pengguna bahwa sistem tidak memahami pertanyaan. Pengaturan threshold yang tepat sangat penting untuk menyeimbangkan antara tingkat otomasi dan akurasi respons \parencite{cyara2024optimum}.

Analisis threshold membantu menentukan pengaturan yang optimal berdasarkan \textit{Key Performance Indicators} (KPI) bisnis. Organisasi dapat menyesuaikan threshold berdasarkan tiga pendekatan utama. Pendekatan pertama adalah menyesuaikan threshold untuk mencapai persentase target respons yang benar. Pendekatan kedua adalah membatasi persentase maksimal respons yang salah. Pendekatan ketiga adalah mengoptimalkan trade-off antara tingkat otomasi dan risiko kesalahan berdasarkan toleransi bisnis \parencite{cyara2024optimum,genesys2023threshold}.

\subsection{Evaluasi Performa \textit{Intent Classification}}

Evaluasi performa klasifikasi \textit{intent} dilakukan dengan mengukur beberapa metrik kunci yang mencerminkan akurasi dan keandalan sistem dalam memprediksi niat pengguna. Evaluasi yang komprehensif memastikan bahwa sistem dapat beroperasi sesuai dengan standar yang ditetapkan dan memberikan pengalaman pengguna yang optimal.

\subsubsection{Metrik Evaluasi Klasifikasi Intent Multi-Kelas}

Model klasifikasi intent dalam penelitian ini dievaluasi menggunakan metrik standar yang disesuaikan untuk masalah klasifikasi multi-kelas (38 kelas intent yang saling eksklusif), bukan multi-label. Pendekatan evaluasi harus mengakomodasi bahwa setiap kueri pengguna hanya dapat diklasifikasikan ke dalam satu kelas yang benar, sehingga metrik klasifikasi biner harus diperluas menggunakan strategi \textit{one-vs-all} dan agregasi yang tepat \parencite{Grandini2020MultiClassMetrics,EvidentlyMultiClass}.

Perbedaan utama antara klasifikasi multi-kelas dan biner adalah pada interpretasi elemen matriks kebingungan (\textit{confusion matrix}) dan agregasi metrik per-kelas. Untuk 38 kelas, matriks kebingungan berukuran 38Ã—38, dengan setiap baris merepresentasikan kelas hasil prediksi dan setiap kolom merepresentasikan kelas sebenarnya. Prediksi yang benar terletak pada diagonal utama, sedangkan prediksi salah tersebar pada sel off-diagonal. Pola sebaran kesalahan pada matriks kebingungan multi-kelas memberikan wawasan berharga tentang persamaan atau kebingungan antar kelas, misalnya prediksi kelas ``Penjualan-Bulanan'' yang sering disalahklasifikasikan sebagai ``Penjualan-Triwulanan'' menunjukkan kesamaan semantik yang perlu ditangani dengan aturan atau templat yang lebih jelas \parencite{EvidentlyMultiClass}.

Dalam menghitung metrik per kelas-\(i\), masalah klasifikasi multi-kelas diubah menjadi masalah biner secara virtual melalui pendekatan \textit{one-vs-all}:\parencite{Grandini2020MultiClassMetrics}

\begin{itemize}
    \item \textbf{TP\textsubscript{i}} (\textit{True Positive}): Jumlah sampel kelas-\(i\) yang diprediksi benar sebagai kelas-\(i\) (elemen diagonal matriks kebingungan pada baris dan kolom \(i\))
    
    \item \textbf{FP\textsubscript{i}} (\textit{False Positive}): Jumlah sampel kelas selain \(i\) yang diprediksi sebagai kelas-\(i\) (jumlah seluruh baris \(i\) pada matriks kebingungan kecuali elemen diagonal)
    
    \item \textbf{FN\textsubscript{i}} (\textit{False Negative}): Jumlah sampel kelas-\(i\) yang diprediksi salah sebagai kelas lain (jumlah seluruh kolom \(i\) pada matriks kebingungan kecuali elemen diagonal)
\end{itemize}

Dalam konteks Agregasi Metrik (\textit{Macro-Average} dengan \textit{Micro-Average}) dan adanya 38 kelas dalam sistem ini, diperlukan strategi agregasi untuk memperoleh metrik keseluruhan performa sistem. Dua pendekatan utama yang umum digunakan dalam klasifikasi multi-kelas adalah \textit{macro-averaging} dan \textit{micro-averaging} \parencite{EvidentlyMultiClass}

Pendekatan \textbf{\textit{Micro-Averaging}} ini menghitung metrik (\textit{Precision}, \textit{Recall}, \textit{F1-Score}) untuk setiap kelas secara individual, kemudian mengambil rata-rata aritmatik dari 38 nilai metrik tersebut. Strategi ini memberikan bobot yang sama untuk setiap kelas, sehingga mengutamakan performa pada kelas minoritas dan mencegah kelas mayoritas mendominasi hasil evaluasi \parencite{Grandini2020MultiClassMetrics}.

\begin{align}
\text{Presisi}_{\text{makro}} &= \frac{1}{38}\sum_{i=1}^{38} \text{Presisi}_i \\
\text{Daya Ingat}_{\text{makro}} &= \frac{1}{38}\sum_{i=1}^{38} \text{Daya Ingat}_i \\
\text{Skor-F1}_{\text{makro}} &= \frac{1}{38}\sum_{i=1}^{38} \text{Skor-F1}_i
\end{align}

Pendekatan \textbf{\textit{Micro-Averaging}} menjumlahkan seluruh nilai TP, FP, dan FN dari semua kelas terlebih dahulu, kemudian menghitung metrik secara global berdasarkan jumlahan tersebut. Strategi ini memberikan bobot yang sama untuk setiap sampel, sehingga metrik cenderung didominasi oleh kelas mayoritas dan mengabaikan performa pada kelas minoritas. Untuk klasifikasi multi-kelas, mikro-rata presisi = mikro-rata daya ingat = akurasi\parencite{EvidentlyMultiClass}.

\begin{align}
\text{Presisi}_{\text{mikro}} &= \frac{\sum_{i=1}^{38} \text{TP}_i}{\sum_{i=1}^{38} (\text{TP}_i + \text{FP}_i)} \\
\text{Daya Ingat}_{\text{mikro}} &= \frac{\sum_{i=1}^{38} \text{TP}_i}{\sum_{i=1}^{38} (\text{TP}_i + \text{FN}_i)}
\end{align}

Sebagai pendekatan komplementer, rata-rata tertimbang menghitung metrik per-kelas kemudian mengambil rata-rata yang diboboti oleh jumlah sampel di setiap kelas atau \textbf{\textit{Weighted-Averaging}}. Ini mengakomodasi keseimbangan antara kepedulian terhadap kelas minoritas dan representasi data yang ada \parencite{Grandini2020MultiClassMetrics}.

Sistem menggunakan mekanisme pemberian skor keyakinan (\textit{confidence scoring}) dalam rentang 0 hingga 1 untuk setiap prediksi klasifikasi intent. Tingkat keyakinan tertinggi menunjukkan kelas yang diprediksi oleh sistem. Dalam menentukan apakah prediksi dapat diterima atau perlu klarifikasi, ditetapkan \textit{threshold} pada nilai 0{,}70 \parencite{Santosa2025IndoBERTIntent}:

\begin{itemize}
    \item Jika tingkat keyakinan \(\geq 0{,}70\): Prediksi dianggap ``positif'' dan diterima untuk kelas yang diprediksi; sistem memberikan respons berdasarkan intent yang teridentifikasi
    
    \item Jika tingkat keyakinan \(< 0{,}70\): Prediksi dianggap ``tidak cukup yakin''; sistem meminta klarifikasi kepada pengguna atau menawarkan alternatif intent yang mungkin
\end{itemize}

Dengan demikian, untuk setiap kelas-\(i\), perhitungan TP, FP, dan FN disesuaikan sebagai berikut:

\begin{itemize}
    \item \textbf{TP\textsubscript{i}}: Prediksi kelas-\(i\) dengan tingkat keyakinan \(\geq 0{,}70\) yang ternyata benar
    
    \item \textbf{FP\textsubscript{i}}: Prediksi kelas-\(i\) dengan tingkat keyakinan \(\geq 0{,}70\) yang ternyata salah
    
    \item \textbf{FN\textsubscript{i}}: Sampel kelas-\(i\) yang sebenarnya tidak terprediksi dengan tingkat keyakinan \(\geq 0{,}70\) (termasuk prediksi kelas lain atau tingkat keyakinan rendah yang ditolak sistem)
\end{itemize}

Visualisasi matriks kebingungan untuk 38 kelas memberikan informasi penting tentang pola kesalahan klasifikasi:\parencite{EvidentlyMultiClass} Diagonal utama menampilkan jumlah prediksi benar per kelas, sementara intensitas warna pada sel off-diagonal menunjukkan kelas-kelas mana yang sering dikacaukan satu sama lain. Sebagai contoh, jika ditemukan frekuensi tinggi kesalahan pada sel (Penjualan-Bulanan, Penjualan-Triwulanan), hal ini menunjukkan bahwa dua kelas tersebut memiliki kesamaan semantik yang tinggi dan memerlukan aturan diferensiasi yang lebih spesifik atau contoh pelatihan yang lebih banyak.

Setiap kelas intent memiliki kepentingan strategis yang berbeda dalam konteks bisnis sehingga penelitian ini menggunakan kombinasi metrik berikut \parencite{Grandini2020MultiClassMetrics}:

\begin{itemize}
    \item \textbf{Skor-F1 Makro} sebagai metrik utama: Mengutamakan performa yang merata di semua 38 kelas, memastikan bahwa kelas-kelas minoritas tidak terabaikan
    
    \item \textbf{Skor-F1 Tertimbang} sebagai metrik sekunder: Mempertimbangkan distribusi frekuensi kelas dalam data percobaan, mencerminkan performa yang lebih realistis terhadap data produksi
    
    \item \textbf{Skor-F1 Mikro} sebagai metrik verifikasi: Harus sama dengan akurasi keseluruhan; digunakan untuk memvalidasi perhitungan
    
    \item \textbf{Matriks Kebingungan}: Untuk analisis kualitatif pola kesalahan dan identifikasi peningkatan yang diperlukan pada aturan atau templat tertentu
\end{itemize}

Berdasarkan studi industri dan best-practices untuk sistem klasifikasi intent dalam chatbot bisnis, target performa yang ditetapkan untuk penelitian ini adalah:\parencite{Santosa2025IndoBERTIntent,Ozerova2022IntentModule}

\begin{itemize}
    \item \textbf{Skor-F1 Makro} \(\geq 0{,}85\): Menjamin performa yang baik dan seimbang di seluruh kelas, termasuk kelas-kelas dengan sampel pelatihan yang lebih sedikit
    
    \item \textbf{Skor-F1 Tertimbang} \(\geq 0{,}90\): Menjamin performa yang baik pada kelas-kelas mayoritas yang mewakili mayoritas pertanyaan pengguna dalam praktik
    
    \item \textbf{Skor-F1 Mikro} (Akurasi Keseluruhan) \(\geq 0{,}88\): Target akurasi minimal untuk sistem dapat diterima dalam produksi
    
    \item \textbf{Matriks Kebingungan}: Diharapkan menunjukkan lebih dari 80\% nilai terprediksi berada pada diagonal utama, mengindikasikan bahwa mayoritas prediksi benar
\end{itemize}

Pencapaian target-target ini akan diverifikasi melalui pengujian pada data validasi yang terpisah dan independen dari data pelatihan model, memastikan bahwa hasil evaluasi tidak bias dan representatif terhadap performa sistem pada data baru di lingkungan produksi.

\subsubsection{\textit{Confusion Matrix} dan \textit{Confidence Score Analysis}}

\textit{Confusion matrix} menyediakan visualisasi detail tentang performa klasifikasi untuk setiap kelas \textit{intent}. Analisis \textit{confusion matrix} memungkinkan identifikasi \textit{intent} yang sering salah diklasifikasikan dan pola kesalahan sistematis yang dapat diperbaiki melalui penyesuaian pola atau threshold.

Analisis distribusi \textit{confidence score} penting untuk memahami bagaimana sistem membedakan antara \textit{intent} yang benar (in-domain) dan pertanyaan di luar domain (out-of-domain atau OOD). Sistem yang baik harus menghasilkan \textit{confidence score} tinggi untuk \textit{intent} in-domain dan score rendah untuk OOD \parencite{khosla2022evaluating,zhang2022disentangling}. Masalah \textit{overconfidence}, dengan sistem menghasilkan score tinggi bahkan untuk sampel OOD yang abnormal, perlu dimitigasi melalui kalibrasi model yang tepat.

\subsubsection{Target Performa \textit{Intent Classification}}

Berdasarkan literatur dan praktik industri terkini, target performa yang ditetapkan untuk sistem klasifikasi \textit{intent} berbasis pola mencakup beberapa metrik kunci \parencite{spotintelligence2023intent,svm2025comparative}. \textit{Accuracy} harus mencapai minimal 85\%, yang berarti minimal 85\% dari semua prediksi \textit{intent} harus benar. \textit{F1-Score} ditargetkan minimal 0,80 untuk memastikan keseimbangan antara presisi dan \textit{recall}. \textit{Precision} ditetapkan minimal 0,82, yang berarti minimal 82\% dari \textit{intent} yang diprediksi positif benar-benar positif. \textit{Recall} ditargetkan minimal 0,78, yang berarti minimal 78\% dari \textit{intent} positif aktual berhasil terdeteksi.

Metrik tambahan terkait \textit{confidence score} meliputi \textit{Average Confidence Score} untuk prediksi benar minimal 0,75, dan \textit{Coverage Rate} atau persentase kueri yang mencapai threshold minimal 80\%. \textit{Response Time} harus di bawah 100 milidetik untuk 95\% kueri agar memberikan pengalaman pengguna yang responsif. Pencapaian target-target ini memastikan bahwa sistem klasifikasi \textit{intent} dapat memberikan pengalaman pengguna yang akurat dan responsif, dengan tingkat kesalahan yang minimal dan kemampuan untuk menangani variasi pertanyaan pengguna yang luas.

% ==============================================================================
\section{\textit{Template-Based Natural Language Generation}}
% ==============================================================================
\begin{figure}[H] 
  \centering
  \includegraphics[width=0.46\textwidth,
                   height=0.32\textheight,
                   keepaspectratio]{image/nlg.png}
  \caption{\textit{Natural Language Generation}}
  \label{fig:nlg}
\end{figure}

Bagian ini membahas pendekatan pembangkitan bahasa alami berbasis templat untuk menghasilkan variasi respons dari data internal perusahaan. Pembahasan mencakup konsep NLG berbasis templat, keamanan data dalam implementasi NLG internal, dan evaluasi kualitas teks yang dihasilkan. Pendekatan ini memastikan bahwa respons yang dihasilkan bervariasi dan natural sambil tetap menjaga keamanan dan privasi data perusahaan.

\subsection{Konsep \textit{Natural Language Generation}}

\textit{Natural Language Generation} (NLG) adalah proses menghasilkan teks bahasa alami dari representasi data terstruktur. Dalam konteks \textit{chatbot} Business Intelligence, NLG digunakan untuk mengubah hasil kueri basis data menjadi respons bahasa natural yang mudah dipahami pengguna \parencite{kale2020template}. NLG memungkinkan sistem untuk menyajikan informasi numerik dan data terstruktur dalam format naratif yang lebih mudah dicerna.

Pasar NLP global diproyeksikan tumbuh dari USD 35,43 miliar pada tahun 2024 menjadi USD 438,08 miliar pada tahun 2034, dengan CAGR sebesar 28,6\% \parencite{Precedence2025}. Pertumbuhan eksponensial ini didorong oleh meningkatnya adopsi teknologi AI, \textit{cloud computing}, dan kebutuhan akan analisis data tidak terstruktur di berbagai sektor industri. Pertumbuhan pasar ini menunjukkan relevansi dan urgensi pengembangan sistem NLG untuk mendukung kebutuhan bisnis modern.

\subsection{\textit{Template-Based Natural Language Generation}}

Sistem NLG berbasis templat memetakan input non-linguistik secara langsung ke struktur linguistik yang berisi "celah" atau \textit{placeholders} yang diisi selama output \parencite{vandeemter2005template}. Pendekatan ini menggunakan templat yang telah ditentukan sebelumnya dengan slot yang dapat diisi dengan data aktual dari hasil kueri basis data.

Contoh implementasi templat sederhana mencakup struktur seperti "Penjualan pada \texttt{[bulan]} mencapai \texttt{[nilai]}, meningkat \texttt{[persentase]}\% dari bulan sebelumnya." Sistem kemudian mengisi placeholder dengan data aktual: "Penjualan pada Oktober 2025 mencapai Rp 5,2 miliar, meningkat 15\% dari bulan sebelumnya." \parencite{kale2020template}.

Keuntungan utama pendekatan berbasis templat adalah kontrol penuh atas output, kecepatan generasi yang tinggi, tidak memerlukan dataset pelatihan besar, dan kemudahan maintenance dan modifikasi. Sistem dapat menghasilkan variasi output dengan mendefinisikan multiple templat untuk skenario yang sama, menghindari respons yang monoton \parencite{kapoor2023implementing}.

\subsection{\textit{Template Rewriting} dengan \textit{Pre-trained Language Models}}

Pendekatan lanjutan dalam NLG berbasis templat melibatkan penggunaan model bahasa pre-trained untuk menulis ulang templat sederhana menjadi teks yang lebih koheren dan natural. Metode ini menggabungkan keuntungan templat dengan fleksibilitas model bahasa neural \parencite{kale2020template,rebuffel2020fewshot}.

Arsitektur sistem mencakup tiga tahapan utama. Tahap pertama adalah modul kebijakan menghasilkan sekumpulan tindakan berdasarkan konteks. Tahap kedua adalah templat sederhana mengkonversi setiap tindakan menjadi \textit{utterance} bahasa alami. Tahap ketiga adalah model \textit{encoder-decoder} seperti T5 menulis ulang gabungan \textit{utterances} menjadi respons percakapan yang koheren \parencite{kale2020template}.

Pendekatan ini memungkinkan sistem untuk menghasilkan respons yang secara semantik benar dari templat, kemudian model bahasa memperbaiki koherensi dan kealamian teks. Metode \textit{template rewriting} telah menunjukkan peningkatan sample efficiency yang signifikan dibandingkan metode end-to-end, dengan jumlah templat yang tumbuh linear terhadap jumlah slot dibandingkan pertumbuhan kombinatorial \parencite{rebuffel2020fewshot}.

\subsection{Keamanan dan Privasi Data dalam NLG Internal}

Implementasi NLG dalam lingkungan perusahaan memerlukan pertimbangan keamanan yang ketat untuk melindungi data sensitif dan mencegah kebocoran informasi. Sistem NLG internal harus dirancang dengan prinsip \textit{privacy-by-design} untuk memastikan data perusahaan tetap aman.

\subsubsection{\textit{Differential Privacy} untuk NLG}

\textit{Differential Privacy} (DP) adalah kerangka kerja yang memberikan jaminan privasi secara matematis, memastikan bahwa kontribusi individu dalam dataset dibatasi \parencite{feyisetan2022differential,mireshghallah2022differentially}. Dalam konteks NLG, DP dapat diterapkan melalui dua pendekatan utama. Pendekatan pertama adalah pelatihan model dengan DP-SGD yang menambahkan noise pada gradien selama pelatihan. Pendekatan kedua adalah perturbasi representasi teks yang menambahkan noise pada embedding sebelum generasi.

Parameter privasi $\epsilon$ mengontrol tingkat privasi yang diberikan, dengan nilai $\epsilon$ yang lebih kecil memberikan privasi yang lebih kuat tetapi dapat mengurangi utility model. Implementasi DP dalam NLG memerlukan trade-off yang cermat antara privasi dan kualitas output \parencite{luo2024data}.

\subsubsection{Strategi Keamanan Data dalam NLG}

Implementasi NLG internal yang aman memerlukan beberapa strategi keamanan berlapis \parencite{itconvergence2024generative}. Strategi pertama adalah enkripsi data dengan mengenkripsi data saat transit menggunakan TLS/SSL dan saat istirahat untuk melindungi data sensitif. Strategi kedua adalah kontrol akses yang ketat dengan menerapkan autentikasi dan otorisasi yang robust, menggunakan prinsip \textit{least privilege}. Strategi ketiga adalah data minimization dengan hanya menggunakan data minimum yang diperlukan untuk generasi respons. Strategi keempat adalah \textit{template-based approach} yang membatasi generasi hanya pada templat yang telah diverifikasi, mencegah generasi teks bebas yang dapat membocorkan informasi sensitif.

Strategi kelima adalah audit dan monitoring dengan mencatat semua aktivitas NLG untuk deteksi anomali dan audit keamanan. Strategi terakhir adalah \textit{secure deployment environment} dengan mengamankan infrastruktur server, firewall, dan menjaga software tetap up-to-date \parencite{itconvergence2024generative,aws2024securing}.

\subsubsection{Mitigasi Risiko dalam NLG Internal}

Risiko utama dalam implementasi NLG internal mencakup kebocoran data sensitif melalui output yang dihasilkan, \textit{prompt injection} yang dapat memanipulasi model untuk menghasilkan output tidak diinginkan, dan \textit{data tampering} yang dapat menyebabkan respons AI yang salah. Mitigasi risiko ini memerlukan pendekatan multi-layer \parencite{postgresql2024generative}.

Teknik mitigasi mencakup pemfilteran output untuk memeriksa dan menyensor informasi sensitif sebelum ditampilkan ke pengguna. \textit{Input validation} digunakan untuk memvalidasi semua input dan menolak pola yang mencurigakan. \textit{Template whitelisting} membatasi generasi hanya pada templat yang telah disetujui. Implementasi \textit{role-based access control} (RBAC) memastikan pengguna hanya dapat mengakses data sesuai peran mereka. Regular security assessment dilakukan untuk menguji kerentanan sistem secara berkala \parencite{edpb2025privacy,velotix2025security}.

\subsection{Evaluasi Kualitas \textit{Natural Language Generation}}

Evaluasi kualitas output NLG merupakan aspek kritis dalam memastikan bahwa teks yang dihasilkan memenuhi standar kualitas dan sesuai dengan kebutuhan bisnis. Evaluasi dilakukan menggunakan berbagai metrik otomatis dan penilaian manusia.

\subsubsection{Metrik Evaluasi NLG}

Metrik evaluasi NLG dapat dikategorikan menjadi beberapa jenis berdasarkan karakteristiknya \parencite{nimah2023nlg,gehrmann2022nlgeval}. Kategori pertama adalah metrik berbasis referensi yang membandingkan output dengan teks referensi yang ditulis manusia. Contoh metrik ini adalah BLEU yang mengukur kecocokan \textit{n-gram} antara output dan referensi, dengan formula:

\begin{equation}
\text{BLEU} = BP \times \exp\left(\sum_{n=1}^N w_n \log p_n\right)
\end{equation}

dengan $p_n$ adalah presisi \textit{n-gram} dan $BP$ adalah \textit{brevity penalty}. Metrik lain adalah METEOR yang mempertimbangkan sinonim dan stemming, serta ROUGE yang fokus pada \textit{recall} untuk evaluasi summarization.

Kategori kedua adalah metrik berbasis embedding yang menggunakan representasi neural untuk mengukur kesamaan semantik. Contohnya adalah BERTScore yang menggunakan embedding BERT untuk menghitung kesamaan kontekstual antara tokens output dan referensi \parencite{nimah2023nlg}.

\subsubsection{Metrik \textit{Task-Specific} untuk BI \textit{Chatbot}}

Evaluasi NLG dalam konteks \textit{chatbot} BI memerlukan metrik tambahan yang spesifik untuk domain. Metrik pertama adalah \textit{Factual Accuracy} yang mengukur kebenaran faktual dari informasi numerik dan data yang disajikan dalam respons. Metrik kedua adalah \textit{Coherence} yang menilai koherensi dan alur logis dari respons yang dihasilkan. \textit{Fluency} sebagai metrik ketiga mengukur kealamian dan kelancaran bahasa yang dihasilkan. Metrik keempat adalah \textit{Relevance} yang menilai relevansi respons terhadap konteks kueri. Metrik terakhir adalah \textit{Variability} yang mengukur keragaman respons untuk input yang sama atau serupa \parencite{nimah2023nlg}.

\subsubsection{Target Performa NLG}

Berdasarkan literatur dan praktik industri, target performa yang ditetapkan untuk sistem NLG berbasis templat dalam konteks \textit{chatbot} BI mencakup beberapa kriteria utama \parencite{nimah2023nlg,gehrmann2022nlgeval}. \textit{Factual Accuracy} harus mencapai 100\%, dengan tidak ada kesalahan dalam penyajian data numerik atau faktual. BLEU-4 score ditargetkan minimal 0,60 untuk memastikan kesamaan struktural yang baik dengan referensi natural. \textit{Fluency} (penilaian manusia) harus mencapai minimal 4 dari skala 5, menunjukkan bahasa yang natural dan lancar. \textit{Response Variety} ditargetkan minimal 5 variasi templat per kategori respons untuk menghindari monotonitas. \textit{Generation Time} harus di bawah 50 milidetik untuk menjaga responsivitas sistem.

Pencapaian target-target ini memastikan bahwa sistem NLG dapat menghasilkan respons yang akurat, natural, dan bervariasi sambil tetap menjaga keamanan data internal perusahaan. Evaluasi berkelanjutan dan penyempurnaan templat diperlukan untuk mempertahankan kualitas output yang tinggi.

% ==============================================================================
\section{\textit{Time Series Forecasting}}
% ==============================================================================
\begin{figure}[H] 
  \centering
  \includegraphics[width=0.46\textwidth,
                   height=0.32\textheight,
                   keepaspectratio]{image/timeseries.png}
  \caption{\textit{Time Series Forecasting}}
  \label{fig:tsf}
\end{figure}

Bagian ini membahas konsep peramalan deret waktu, model-model statistik dan pembelajaran mendalam yang digunakan, serta evaluasi performa yang komprehensif. Pembahasan mencakup ARIMA, SARIMA, LSTM, serta perbandingan karakteristik dan performa masing-masing metode. Pemahaman yang mendalam tentang berbagai pendekatan peramalan ini menjadi fondasi penting dalam memilih dan mengimplementasikan model yang sesuai dengan karakteristik data dan kebutuhan aplikasi.

\subsection{Konsep \textit{Time Series} dan Peramalan}

\textit{Time series} adalah serangkaian titik data yang diindeks dalam urutan waktu, dan peramalan deret waktu adalah proses menggunakan model untuk memprediksi nilai masa depan berdasarkan nilai yang diamati sebelumnya. Peramalan akurat sangat penting untuk perencanaan strategis dan pengambilan keputusan di berbagai domain bisnis.

Pasar \textit{time series forecasting} global mencapai USD 9,62 miliar pada tahun 2023 dan diperkirakan tumbuh menjadi USD 36,9 miliar pada tahun 2032 dengan CAGR sebesar 16,12\% \parencite{WiseGuyReports2024}. Pertumbuhan ini menunjukkan meningkatnya adopsi teknik peramalan deret waktu di berbagai industri untuk mendukung pengambilan keputusan berbasis data, termasuk peramalan permintaan layanan pelanggan, prediksi penjualan, dan analisis tren bisnis. Proyeksi pertumbuhan pasar ini mengindikasikan relevansi dan urgensi pengembangan sistem peramalan yang akurat dan dapat diandalkan.

\subsection{Model ARIMA untuk \textit{Time Series}}

ARIMA (\textit{AutoRegressive Integrated Moving Average}) adalah model statistik populer untuk peramalan deret waktu yang menggabungkan tiga komponen utama. Model ARIMA dinyatakan sebagai ARIMA$(p, d, q)$, dengan $p$ adalah jumlah \textit{lag} observasi dalam model atau \textit{order} dari komponen AR, $d$ adalah jumlah kali deret waktu harus dibedakan untuk menjadi stasioner, dan $q$ adalah ukuran jendela \textit{moving average} atau \textit{order} dari komponen MA.

Persamaan umum model ARIMA dapat ditulis sebagai:
\begin{equation}
\phi(B)(1-B)^d X_t = \theta(B) \epsilon_t
\end{equation}

Dalam persamaan ini, $X_t$ adalah nilai deret waktu pada waktu $t$, $B$ adalah operator \textit{backshift}, $\phi(B)$ adalah polinomial autoregresif, $\theta(B)$ adalah polinomial \textit{moving average}, $\epsilon_t$ adalah \textit{white noise error term}, dan $d$ adalah \textit{order} pembedaan.

Model ARIMA efektif untuk data yang menunjukkan pola linear dan stasioner setelah pembedaan. Kelebihan utama ARIMA adalah interpretabilitas yang tinggi dan tidak memerlukan dataset besar untuk pelatihan, menjadikannya pilihan yang baik untuk aplikasi dengan data terbatas atau ketika transparansi model menjadi prioritas. Karakteristik ini membuat ARIMA tetap relevan meskipun telah muncul metode-metode yang lebih kompleks.

\subsection{Model SARIMA untuk Data Musiman}

SARIMA (\textit{Seasonal ARIMA}) adalah ekstensi dari ARIMA yang dirancang untuk menangani data deret waktu musiman dengan lebih baik. SARIMA dinyatakan sebagai SARIMA$(p, d, q)(P, D, Q, m)$, dengan $(p, d, q)$ adalah parameter non-musiman dan $(P, D, Q)$ adalah parameter musiman, serta $m$ sebagai jumlah langkah waktu untuk satu siklus musiman penuh.

Persamaan SARIMA dapat ditulis sebagai:
\begin{equation}
\phi(B)\Phi(B^m)(1-B)^d(1-B^m)^D X_t = \theta(B)\Theta(B^m) \epsilon_t
\end{equation}

Dalam persamaan ini, $\Phi(B^m)$ dan $\Theta(B^m)$ adalah polinomial autoregresif dan \textit{moving average} musiman.

SARIMA sangat cocok untuk data yang menunjukkan pola berulang pada interval waktu tetap, seperti penjualan bulanan yang cenderung meningkat pada bulan-bulan tertentu atau permintaan layanan pelanggan yang bervariasi mengikuti siklus musiman. Penambahan komponen musiman memungkinkan model untuk menangkap dan memprediksi pola periodik yang tidak dapat ditangani oleh ARIMA standar. Kemampuan ini menjadikan SARIMA pilihan yang tepat untuk data bisnis yang umumnya menunjukkan pola musiman yang kuat.

\subsection{Model LSTM untuk \textit{Time Series}}

LSTM (\textit{Long Short-Term Memory}) adalah jenis jaringan saraf tiruan berulang yang dirancang khusus untuk mengatasi masalah \textit{vanishing gradient} dalam RNN tradisional. LSTM mampu mempelajari dependensi jangka panjang dalam data sekuens, menjadikannya sangat efektif untuk peramalan deret waktu.

Arsitektur LSTM terdiri dari sel memori dan tiga gerbang utama. Gerbang lupa menentukan informasi apa dari sel memori yang harus dibuang. Gerbang masukan menentukan informasi baru apa yang akan disimpan dalam sel memori. Gerbang keluaran menentukan bagian mana dari sel memori yang akan menjadi keluaran.

Persamaan matematis LSTM dapat dituliskan sebagai berikut:
\begin{align}
f_t &= \sigma(W_f \cdot [h_{t-1}, x_t] + b_f) \\
i_t &= \sigma(W_i \cdot [h_{t-1}, x_t] + b_i) \\
\tilde{C}_t &= \tanh(W_C \cdot [h_{t-1}, x_t] + b_C) \\
C_t &= f_t * C_{t-1} + i_t * \tilde{C}_t \\
o_t &= \sigma(W_o \cdot [h_{t-1}, x_t] + b_o) \\
h_t &= o_t * \tanh(C_t)
\end{align}

Dalam persamaan-persamaan ini, $f_t$ adalah gerbang lupa, $i_t$ adalah gerbang masukan, $o_t$ adalah gerbang keluaran, $C_t$ adalah keadaan sel, $h_t$ adalah keadaan tersembunyi, $\sigma$ adalah fungsi sigmoid, serta $W$ dan $b$ adalah parameter bobot dan \textit{bias} yang dipelajari selama pelatihan.

LSTM unggul dalam menangani data deret waktu yang kompleks dengan dependensi jangka panjang dan pola non-linear. Kemampuan model untuk "mengingat" informasi relevan dari masa lalu yang jauh membuat LSTM sangat efektif untuk aplikasi seperti peramalan permintaan layanan pelanggan yang dipengaruhi oleh berbagai faktor temporal yang kompleks. Fleksibilitas dan kemampuan pembelajaran LSTM menjadikannya pilihan yang powerful untuk aplikasi dengan data yang kaya dan pola yang kompleks.

\subsection{Perbandingan ARIMA, SARIMA, dan LSTM}

Perbandingan komprehensif antara metode-metode peramalan deret waktu memberikan wawasan penting dalam memilih pendekatan yang sesuai untuk aplikasi spesifik. Pemahaman tentang kelebihan dan keterbatasan masing-masing metode memungkinkan pengambilan keputusan yang informed dalam pemilihan model.

\subsubsection{Studi Empiris Perbandingan Model}

Studi empiris menunjukkan bahwa algoritma berbasis \textit{deep learning} seperti LSTM mengungguli algoritma tradisional seperti ARIMA \parencite{siami2018comparison}. Rata-rata pengurangan tingkat kesalahan yang diperoleh oleh LSTM adalah antara 84-87\% bila dibandingkan dengan ARIMA, menunjukkan keunggulan LSTM atas ARIMA dalam menangani data dengan pola kompleks dan non-linear.

Perbandingan ketiga model untuk prediksi profit menunjukkan akurasi sebesar 93,84\% untuk ARIMA, 94,378\% untuk SARIMA, dan 97,01\% untuk LSTM \parencite{sirisha2022profit}. LSTM memberikan akurasi tertinggi untuk prediksi profit. Hasil metrik kinerja menunjukkan bahwa ARIMA memiliki MAE = 33,05, RMSE = 39,28, dan $R^2$ = 0,912, SARIMA memiliki MAE = 31,48, RMSE = 38,06, dan $R^2$ = 0,921, sementara LSTM memiliki MAE = 28,63, RMSE = 34,91, dan $R^2$ = 0,937 \parencite{sirisha2022profit}.

Perbandingan ARIMA, LSTM, dan GRU menunjukkan bahwa LSTM mengungguli model lain dengan MAPE sebesar 10,76\%, diikuti oleh ARIMA pada 11,23\% dan GRU pada 11,47\% \parencite{sunendar2025comparison}. Temuan ini mengonfirmasi bahwa LSTM memiliki kemampuan generalisasi terbaik, terutama dalam menangani lompatan mendadak dan struktur temporal yang kompleks. Konsistensi hasil dari berbagai studi ini memperkuat posisi LSTM sebagai metode yang powerful untuk peramalan deret waktu yang kompleks.

\subsubsection{Model Hibrida ARIMA-LSTM}

Model hibrida ARIMA-LSTM menggabungkan keunggulan komplementer dari kedua pendekatan, menyediakan kerangka kerja yang lebih kuat dan akurat untuk peramalan deret waktu \parencite{alharbi2025prediction}. ARIMA, dikenal karena efektivitasnya dalam mengidentifikasi pola linear dan tren, membentuk fondasi model dengan menangani elemen stasioner dan linear dari deret waktu.

LSTM dirancang khusus untuk menangani dependensi non-linear dan hubungan temporal jangka panjang dalam data. Dengan menggabungkan kedua pendekatan ini, model hibrida ARIMA-LSTM mendapatkan manfaat dari kemampuan pemodelan linear ARIMA dan pengenalan pola non-linear LSTM \parencite{alharbi2025prediction}. Integrasi ini memungkinkan model untuk menangkap dengan efektif baik fluktuasi jangka pendek maupun tren jangka panjang, menghasilkan prediksi yang lebih akurat dan robust terhadap berbagai kondisi data. Pendekatan hibrida ini memberikan solusi yang optimal dengan memanfaatkan kekuatan masing-masing metode.

\subsubsection{Perbandingan Karakteristik Model}

Studi komparatif menunjukkan bahwa LSTM lebih baik dalam menangkap pola musiman dan tren jangka panjang dibandingkan dengan ARIMA dan SARIMA untuk data konsumsi energi \parencite{dubey2021study}. Tabel~\ref{tab:forecasting-comparison} menunjukkan perbandingan karakteristik metode peramalan berdasarkan sintesis dari berbagai penelitian.

\begin{table}[htbp]
\centering
\caption{Perbandingan Metode Peramalan Deret Waktu}
\label{tab:forecasting-comparison}
\small
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{|p{2cm}|p{4.2cm}|p{4.2cm}|p{2cm}|}
\hline
\textbf{Metode} & \textbf{Kelebihan} & \textbf{Kekurangan} & \textbf{MAPE Tipikal} \tabularnewline
\hline
ARIMA &
-- \textit{Interpretable}

-- Tidak memerlukan data besar

-- Cepat untuk dilatih
&
-- Sulit menangkap pola non-\textit{linear}

-- Memerlukan data stasioner
&
\centering 11--15\% \tabularnewline
\hline
SARIMA &
-- Menangani musiman

-- \textit{Interpretable}

-- Baik untuk pola periodik
&
-- Memerlukan pengetahuan domain

-- Parameter banyak
&
\centering 10--14\% \tabularnewline
\hline
LSTM &
-- Menangkap dependensi jangka panjang

-- Baik untuk pola kompleks

-- Tidak perlu stasioner
&
-- Memerlukan data besar

-- \textit{Black box}

-- Lama untuk dilatih
&
\centering 8--12\% \tabularnewline
\hline
\end{tabular}
\end{table}

Perbandingan ini menunjukkan bahwa pemilihan model harus disesuaikan dengan karakteristik data dan kebutuhan aplikasi spesifik. ARIMA dan SARIMA cocok untuk aplikasi yang memerlukan interpretabilitas tinggi dan memiliki data terbatas, sementara LSTM lebih sesuai untuk aplikasi dengan data yang berlimpah dan pola kompleks yang sulit dimodelkan dengan pendekatan statistik tradisional. Pemahaman tentang trade-off ini penting dalam membuat keputusan yang tepat untuk implementasi sistem peramalan.

\subsection{Evaluasi Performa \textit{Time Series Forecasting}}

Evaluasi performa model peramalan deret waktu merupakan langkah kritis untuk memastikan bahwa prediksi yang dihasilkan memiliki akurasi yang memadai untuk mendukung pengambilan keputusan bisnis. Evaluasi yang komprehensif mencakup berbagai metrik dan teknik analisis yang saling melengkapi untuk memberikan gambaran menyeluruh tentang kualitas model.

\subsubsection{Metrik Evaluasi \textit{Forecasting}}

Model peramalan deret waktu dievaluasi menggunakan tiga metrik utama yang masing-masing memberikan perspektif berbeda tentang akurasi prediksi \parencite{sirisha2022profit,sunendar2025comparison}. Metrik pertama adalah \textit{Mean Absolute Error} (MAE) yang mengukur rata-rata dari nilai absolut kesalahan:

\begin{equation}
\text{MAE} = \frac{1}{n} \sum_{i=1}^n |y_i - \hat{y}_i|
\end{equation}

MAE memberikan ukuran rata-rata kesalahan absolut tanpa mempertimbangkan arah kesalahan, baik positif maupun negatif. Metrik ini mudah diinterpretasikan dan robust terhadap \textit{outliers}.

Metrik kedua adalah \textit{Root Mean Square Error} (RMSE) yang mengukur akar kuadrat dari rata-rata kesalahan kuadrat:
\begin{equation}
\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2}
\end{equation}

RMSE memberikan penalti yang lebih besar untuk kesalahan yang lebih besar karena proses kuadrat. Metrik ini lebih sensitif terhadap \textit{outliers} dibandingkan MAE dan sering digunakan ketika kesalahan besar sangat tidak diinginkan.

Metrik ketiga adalah \textit{Mean Absolute Percentage Error} (MAPE) yang mengukur rata-rata persentase kesalahan absolut:
\begin{equation}
\text{MAPE} = \frac{100\%}{n} \sum_{i=1}^n \left|\frac{y_i - \hat{y}_i}{y_i}\right|
\end{equation}

Dalam formula-formula ini, $y_i$ adalah nilai aktual, $\hat{y}_i$ adalah nilai prediksi, dan $n$ adalah jumlah observasi. MAPE memberikan ukuran kesalahan dalam bentuk persentase, memudahkan interpretasi dan perbandingan antar dataset dengan skala berbeda. Namun, MAPE tidak dapat digunakan ketika nilai aktual bernilai nol dan sangat sensitif terhadap nilai aktual yang kecil.

\subsubsection{\textit{Residual Analysis}}

Analisis residual merupakan komponen penting dalam evaluasi model peramalan untuk memastikan bahwa asumsi model terpenuhi dan tidak ada pola sistematis yang tersisa dalam kesalahan prediksi. Analisis ini mencakup beberapa teknik yang saling melengkapi.

Teknik pertama adalah plot residual terhadap waktu untuk mendeteksi pola temporal yang mungkin tersisa. Residual yang baik harus terdistribusi secara acak di sekitar nol tanpa tren atau pola musiman yang jelas. Teknik kedua adalah uji normalitas residual menggunakan uji Shapiro-Wilk untuk menguji apakah residual mengikuti distribusi normal. Normalitas residual mengindikasikan bahwa model telah menangkap sebagian besar struktur dalam data. Teknik ketiga adalah uji autokorelasi residual menggunakan uji Ljung-Box untuk mendeteksi autokorelasi dalam residual. Autokorelasi yang signifikan mengindikasikan bahwa masih ada informasi temporal yang belum ditangkap oleh model. Ketiga teknik ini memberikan pemeriksaan komprehensif terhadap kualitas model dan asumsi yang mendasarinya.

\subsubsection{\textit{Cross-Validation} untuk \textit{Time Series}}

Validasi silang khusus untuk deret waktu (\textit{time series cross-validation}) menggunakan pendekatan \textit{walk-forward validation} atau \textit{expanding window} untuk menghindari kebocoran informasi dari masa depan ke masa lalu. Pendekatan ini membagi data secara berurutan dan melatih model pada data historis untuk memprediksi periode mendatang, mensimulasikan kondisi \textit{deployment} aktual dengan model harus memprediksi masa depan berdasarkan data masa lalu. Validasi silang deret waktu memberikan estimasi performa yang lebih realistis dibandingkan validasi silang standar yang dapat menghasilkan overfitting karena kebocoran informasi temporal.

\subsubsection{Target Performa \textit{Forecasting}}

Berdasarkan literatur dan praktik industri terkini, target performa yang ditetapkan untuk model peramalan deret waktu dalam konteks permintaan layanan pelanggan mencakup beberapa kriteria utama \parencite{siami2018comparison,sunendar2025comparison,sirisha2022profit}. MAPE harus mencapai maksimal 15\%, dengan kesalahan persentase rata-rata maksimal 15\% yang dianggap dapat diterima untuk sebagian besar aplikasi bisnis. RMSE harus seminimal mungkin dan dibandingkan dengan standar deviasi data aktual, dengan RMSE yang kurang dari setengah standar deviasi data aktual umumnya dianggap baik. MAE harus seminimal mungkin dan diinterpretasikan dalam konteks skala data, dengan MAE yang secara signifikan lebih kecil dari mean nilai aktual menunjukkan performa yang baik.

\textit{Coefficient of Determination} ($R^2$) harus mencapai minimal 0,85, yang berarti model harus dapat menjelaskan minimal 85\% variansi dalam data. Residual harus lulus uji Ljung-Box dengan p-value lebih besar dari 0,05, mengindikasikan tidak ada autokorelasi signifikan yang tersisa. Pencapaian target-target ini memastikan bahwa model peramalan dapat memberikan prediksi yang akurat dan dapat diandalkan untuk mendukung perencanaan dan pengambilan keputusan strategis terkait permintaan layanan pelanggan.

% ==============================================================================
\section{Integrasi Sistem dan Penelitian Terkait}
% ==============================================================================

Bagian ini membahas integrasi berbagai komponen teknologi dalam sistem \textit{chatbot} Business Intelligence, penelitian terkait arsitektur \textit{chatbot} hibrida, serta aplikasi pembelajaran mesin untuk prediksi \textit{customer churn}. Pembahasan mencakup strategi integrasi, arsitektur sistem, dan studi kasus implementasi yang menunjukkan efektivitas pendekatan terintegrasi. Pemahaman tentang integrasi sistem menjadi kunci dalam mengembangkan solusi yang komprehensif dan koheren.

\subsection{Arsitektur \textit{Chatbot} Hibrida}

Arsitektur \textit{chatbot} hibrida menggabungkan pendekatan berbasis aturan (\textit{rule-based}) dengan kemampuan kecerdasan buatan untuk menciptakan sistem yang seimbang antara kontrol deterministik dan fleksibilitas adaptif. Pendekatan hibrida ini memungkinkan sistem untuk menangani berbagai jenis kueri dengan efisiensi optimal sambil mempertahankan akurasi dan keamanan data.

\subsubsection{Komponen Arsitektur \textit{Chatbot} Modern}

Arsitektur \textit{chatbot} modern terdiri dari lima komponen utama yang bekerja secara terintegrasi \parencite{huang2021chatbot,xenonstack2023chatbots}. Komponen pertama adalah antarmuka pengguna (\textit{User Interface}) yang memungkinkan pengguna berkomunikasi dengan \textit{chatbot} melalui platform perpesanan atau antarmuka berbasis web. Komponen ini dapat mencakup konversi \textit{speech-to-text} untuk input suara dan \textit{text-to-speech} untuk output suara.

Komponen kedua adalah modul pemahaman bahasa alami (\textit{Natural Language Understanding}) yang bertanggung jawab untuk menginterpretasikan masukan pengguna dan mengekstraksi \textit{intent} serta entitas. Komponen ketiga adalah manajemen dialog (\textit{Dialogue Management}) yang mengelola alur percakapan, memelihara konteks, dan menentukan tindakan yang sesuai berdasarkan \textit{intent} yang teridentifikasi. Komponen keempat adalah \textit{backend} yang terhubung dengan basis data, sistem eksternal, dan API untuk mengambil informasi yang diperlukan. Komponen terakhir adalah pembangkitan respons (\textit{Response Generation}) yang menghasilkan output bahasa alami berdasarkan data yang diperoleh dan konteks percakapan \parencite{huang2021chatbot}.

\subsubsection{\textit{Chatbot} Hibrida dengan AI dan \textit{Human Oversight}}

Pendekatan \textit{chatbot} hibrida yang mengintegrasikan model NLP ringan dengan pengawasan manusia telah menunjukkan efektivitas tinggi dalam konteks pendidikan tinggi. Kerangka kerja hibrida ini menggunakan pohon keputusan, pemetaan \textit{intent}, dan alur percakapan terstruktur untuk mengotomatisasi tugas rutin sambil mempertahankan dukungan kontekstual dan empatik melalui protokol eskalasi dinamis \parencite{bamurange2025hybrid}.

Implementasi kerangka kerja hibrida pada Universitas Kigali menunjukkan akurasi 85\% dalam menyelesaikan pertanyaan akademik dan mengurangi beban kerja staf sebesar 30\% dalam simulasi \parencite{bamurange2025hybrid}. Inovasi kunci mencakup fungsi \textit{offline} untuk pengaturan dengan keterbatasan sumber daya, pemeriksaan kesesuaian budaya untuk menginterpretasikan kueri tidak langsung, dan strategi mitigasi bias yang selaras dengan pedoman etika. Temuan ini menekankan kelayakan platform \textit{no-code} untuk deployment yang scalable, menekankan keseimbangan antara otomasi dan intervensi manusia.

\subsubsection{Pasar \textit{Chatbot} AI Global}

Pasar \textit{chatbot} berbasis AI global akan melebihi USD 3 miliar pada tahun 2025, dengan pasar \textit{chatbot} hibrida suara dan teks mencapai USD 534 juta secara global pada tahun yang sama \parencite{businesswire2020chatbot}. Penggunaan \textit{Conversational AI} akan merevolusi manajemen hubungan pelanggan, dengan lebih dari 50\% kueri pelanggan dapat dikelola melalui \textit{chatbot} berbasis AI. Pertumbuhan eksponensial ini menunjukkan bahwa \textit{chatbot} berbasis AI akan menjadi norma pada tahun 2025 ketika pusat kontak meningkatkan otomasi.

Transformasi dari antarmuka operasional tradisional ke antarmuka percakapan akan mengubah secara dramatis ekspektasi tentang bagaimana manusia berkomunikasi, mengonsumsi konten, menggunakan aplikasi, dan terlibat dalam perdagangan \parencite{businesswire2020chatbot}. Transformasi ini akan berdampak pada hampir setiap aspek operasi pemasaran dan penjualan untuk setiap vertikal industri, termasuk sektor Business Intelligence.

\subsection{Integrasi \textit{Pattern-Based Intent} dan \textit{Template-Based} NLG}

Integrasi antara klasifikasi \textit{intent} berbasis pola dan pembangkitan bahasa alami berbasis templat menciptakan alur kerja yang efisien dan aman untuk sistem \textit{chatbot} Business Intelligence. Integrasi ini memastikan bahwa kueri pengguna diproses dengan akurat sambil respons yang dihasilkan tetap bervariasi dan natural.

\subsubsection{Alur Kerja Terintegrasi}

Alur kerja sistem terintegrasi dimulai dengan penerimaan kueri pengguna melalui antarmuka \textit{chatbot}. Modul klasifikasi \textit{intent} berbasis pola melakukan pencocokan kueri dengan pola yang telah didefinisikan dan menghitung \textit{confidence score}. Jika skor mencapai threshold yang ditentukan, sistem mengidentifikasi \textit{intent} dan mengekstraksi parameter yang diperlukan untuk eksekusi kueri basis data.

Setelah \textit{intent} teridentifikasi, sistem mengeksekusi kueri terhadap \textit{data warehouse} untuk mengambil data internal yang relevan. Data yang diperoleh kemudian diteruskan ke modul NLG berbasis templat. Modul NLG memilih templat yang sesuai berdasarkan jenis \textit{intent} dan konteks, mengisi \textit{placeholder} dengan data aktual, dan menghasilkan respons bahasa alami. Untuk meningkatkan kealamian, sistem dapat menggunakan model \textit{template rewriting} seperti T5 untuk memperbaiki koherensi dan variasi respons.

Pendekatan terintegrasi ini memastikan bahwa sistem dapat merespons dengan cepat untuk kueri terstruktur sambil menjaga keamanan data internal melalui pembatasan pada templat yang telah diverifikasi. Kontrol penuh atas output NLG mencegah kebocoran informasi sensitif dan memastikan bahwa respons tetap konsisten dengan kebijakan perusahaan.

\subsubsection{Evaluasi Sistem Terintegrasi}

Evaluasi sistem terintegrasi memerlukan pengukuran performa pada setiap komponen serta performa sistem secara keseluruhan. Metrik evaluasi mencakup akurasi klasifikasi \textit{intent}, kualitas respons NLG, waktu respons end-to-end, dan kepuasan pengguna. Sistem yang baik harus mencapai akurasi klasifikasi \textit{intent} minimal 85\%, \textit{factual accuracy} NLG 100\%, dan waktu respons total di bawah 500 milidetik untuk 95\% kueri.

Pengujian integrasi juga mencakup skenario pengujian end-to-end yang mensimulasikan percakapan nyata, pengujian beban untuk memastikan sistem dapat menangani volume kueri tinggi, dan pengujian keamanan untuk memverifikasi bahwa tidak ada kebocoran data sensitif. Pendekatan evaluasi holistik ini memastikan bahwa sistem tidak hanya berfungsi dengan baik pada tingkat komponen individual, tetapi juga memberikan pengalaman pengguna yang koheren dan memuaskan.

\subsection{\textit{Customer Churn Prediction} dengan \textit{Machine Learning}}

Prediksi \textit{customer churn} adalah aplikasi pembelajaran mesin yang kritis dalam analisis bisnis pelanggan, memungkinkan organisasi untuk mengidentifikasi pelanggan berisiko dan mengambil tindakan retensi proaktif. Integrasi prediksi \textit{churn} ke dalam sistem \textit{chatbot} BI memberikan aksesibilitas yang mudah terhadap \textit{insight} prediktif bagi pengguna internal.

\subsubsection{Model \textit{Machine Learning} untuk \textit{Churn Prediction}}

Beberapa model pembelajaran mesin telah terbukti efektif untuk prediksi \textit{customer churn} \parencite{pecan2024churn,kumar2024xgboost}. Model \textit{Logistic Regression} dikenal karena kesederhanaannya dan kemudahan interpretasi, cocok untuk kasus dengan hubungan linear antara fitur dan \textit{churn}. Model \textit{Random Forest} memberikan akurasi superior dalam skenario kompleks dengan kemampuan menangani hubungan non-linear dan interaksi antar variabel. Model \textit{Gradient Boosting Machines} (GBM) dan XGBoost sangat efektif dalam menangkap hubungan kompleks dan non-linear dengan membangun pohon secara sekuensial untuk memperbaiki kesalahan.

Studi terbaru menunjukkan bahwa XGBoost adalah algoritma paling efisien untuk prediksi \textit{churn} pelanggan \parencite{kumar2024xgboost}. Model hibrida berbasis jaringan saraf yang mengkombinasikan \textit{Multi-Head Self-Attention}, BiLSTM, dan CNN telah diusulkan untuk meningkatkan ekstraksi fitur kompleks dan dependensi temporal \parencite{nature2024hybrid}. Model CCP-Net ini menggunakan algoritma sampling ADASYN untuk menyeimbangkan ukuran sampel pelanggan yang \textit{churn} dan tidak \textit{churn}, mengatasi dampak negatif ketidakseimbangan sampel pada performa model.

\subsubsection{Evaluasi Model \textit{Churn Prediction}}

Evaluasi model prediksi \textit{churn} menggunakan metrik standar pembelajaran mesin dengan pertimbangan khusus untuk ketidakseimbangan kelas. Metrik evaluasi mencakup \textit{Accuracy}, \textit{Precision}, \textit{Recall}, \textit{F1-Score}, dan \textit{ROC-AUC}. Studi \textcite{computing2025churn} menunjukkan bahwa \textit{Random Forest} mencapai akurasi 90,30\% dengan semua atribut dan 90,90\% dengan dataset atribut yang dikurangi, mengindikasikan bahwa dataset dengan atribut yang dikurangi dapat berguna untuk tugas prediksi \textit{churn}.

Studi lain menunjukkan bahwa \textit{Random Forest} mencapai akurasi 90,30\%, presisi 89,5\%, \textit{recall} 91,2\%, dan \textit{ROC-AUC} 0,94 \parencite{infosains2024churn}. Model dengan \textit{hyperparameter tuning} menggunakan teknik seperti SMOTE, ENN, dan K-fold cross-validation menghasilkan peningkatan performa yang signifikan. Penggunaan dataset dengan atribut yang dikurangi tidak hanya meningkatkan akurasi tetapi juga mengurangi kompleksitas komputasi, menjadikannya lebih praktis untuk deployment produksi.

\subsubsection{Fitur-fitur untuk \textit{Churn Prediction}}

Fitur-fitur yang umum digunakan untuk prediksi \textit{churn} mencakup karakteristik demografis dan behavioral pelanggan \parencite{pecan2024churn,computing2025churn}. Karakteristik demografis meliputi usia, jenis kelamin, lokasi geografis, dan tingkat pendapatan. Karakteristik behavioral mencakup \textit{tenure} atau lama berlangganan, frekuensi transaksi, nilai transaksi rata-rata, variasi produk yang dibeli, dan pola penggunaan layanan.

Fitur tambahan yang relevan adalah riwayat keluhan atau masalah layanan, tingkat keterlibatan dengan program loyalitas, responsivitas terhadap kampanye pemasaran, dan perubahan pola penggunaan dari waktu ke waktu. \textit{Feature engineering} yang efektif, termasuk pembuatan fitur agregat temporal dan fitur interaksi, dapat meningkatkan performa model secara signifikan.

\subsubsection{\textit{Real-Time Churn Prediction}}

Pendekatan tradisional prediksi \textit{churn} menggunakan pelatihan berbasis \textit{batch} dengan dataset tetap yang dikumpulkan pada interval periodik. Pendekatan ini gagal menangkap sifat dinamis industri dengan preferensi pelanggan yang berubah cepat \parencite{thesai2025realtime}. Model RCE (\textit{Real-time Continual learning-based Enhancement}) telah diusulkan untuk mengatasi keterbatasan ini dengan memanfaatkan pendekatan pengembangan \textit{event-driven} untuk prediksi \textit{churn} waktu nyata.

RCE menggunakan mekanisme \textit{continual learning} berbasis \textit{replay}, memungkinkannya beradaptasi dengan perilaku pelanggan baru sambil mengurangi efek \textit{catastrophic forgetting}. Berbeda dengan model berbasis \textit{batch} tradisional, RCE memproses aliran data yang berkelanjutan, memungkinkan bisnis untuk bereaksi dengan cepat terhadap perubahan pasar \parencite{thesai2025realtime}. Pendekatan ini memastikan bahwa pembuat keputusan menerima \textit{insight} terkini ketika perilaku pelanggan berkembang, meningkatkan efektivitas strategi retensi.

\subsection{Integrasi \textit{Churn Prediction} dengan \textit{Chatbot} BI}

Integrasi model prediksi \textit{churn} ke dalam sistem \textit{chatbot} Business Intelligence memungkinkan pengguna internal untuk dengan mudah mengakses analisis risiko \textit{churn} melalui antarmuka percakapan yang intuitif. Pengguna dapat mengajukan kueri seperti "Tampilkan pelanggan berisiko tinggi \textit{churn} bulan ini" atau "Analisis faktor utama yang mempengaruhi \textit{churn} di segmen premium", dan sistem akan mengeksekusi model prediksi serta menyajikan hasil dalam format bahasa alami yang mudah dipahami.

Integrasi ini memfasilitasi pengambilan keputusan proaktif untuk retensi pelanggan, seperti penawaran promosi khusus atau peningkatan layanan untuk pelanggan berisiko tinggi. Sistem dapat menghasilkan rekomendasi tindakan berdasarkan faktor-faktor yang berkontribusi terhadap risiko \textit{churn} individu pelanggan. \textit{Dashboard} interaktif dapat menampilkan distribusi risiko \textit{churn} berdasarkan segmen pelanggan, tren \textit{churn} dari waktu ke waktu, dan efektivitas kampanye retensi yang telah dilaksanakan.

Aksesibilitas informasi prediktif melalui antarmuka percakapan menurunkan \textit{barrier} untuk adopsi analitik prediktif di kalangan pengguna non-teknis. Pengguna bisnis tanpa latar belakang \textit{data science} dapat memanfaatkan model pembelajaran mesin yang canggih untuk mendukung keputusan mereka tanpa perlu memahami detail teknis implementasi model. Demokratisasi akses terhadap \textit{insight} prediktif ini meningkatkan \textit{data literacy} organisasi dan mempercepat transformasi menuju organisasi \textit{data-driven}.